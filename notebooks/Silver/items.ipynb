{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fbcb74-2471-425b-97c0-7bee2e7c81ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "load raw sales csv"
    }
   },
   "outputs": [],
   "source": [
    "item_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"multiLine\", True)\n",
    "    .csv(\"/mnt/raw-bronze/item.csv\")\n",
    ")\n",
    "#display(item_df)\n",
    "item_df.createOrReplaceTempView(\"items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bbeb94c-900e-4d65-a9f4-9ceb82f9a975",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.columnPruning\", \"false\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9e10f28-e37d-4368-a603-83a8a01e5ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, when, lit, regexp_extract, upper\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Size Engineering (Ensures input columns exist) ---\n",
    "item_df = item_df.withColumn(\n",
    "    \"size_value_str\",\n",
    "    regexp_extract(col(\"size\"), r'(\\d+\\.?\\d*)\\s*([A-Za-z#]+)', 1)\n",
    ")\n",
    "item_df = item_df.withColumn(\n",
    "    \"size_value\",\n",
    "    col(\"size_value_str\").cast(DoubleType())\n",
    ").drop(\"size_value_str\")\n",
    "\n",
    "item_df = item_df.withColumn(\n",
    "    \"size_unit\",\n",
    "    upper(regexp_extract(col(\"size\"), r'(\\d+\\.?\\d*)\\s*([A-Za-z#]+)', 2))\n",
    ")\n",
    "\n",
    "# Data Cleaning Fix (for StringIndexer) ---\n",
    "item_df = item_df.withColumn(\n",
    "    \"size_unit\",\n",
    "    when(col(\"size_unit\") == \"\", lit(\"UNKNOWN\")).otherwise(col(\"size_unit\"))\n",
    ")\n",
    "item_df = item_df.fillna(\"UNKNOWN\", subset=[\"size_unit\", \"brand\", \"type\"])\n",
    "\n",
    "# Defensive Drop of Any Old Indexed/Encoded Columns ---\n",
    "cols_to_drop = [\n",
    "    \"size_unit_indexed\",\n",
    "    \"size_unit_encoded\",\n",
    "    \"brand_indexed\",\n",
    "    \"type_indexed\"  # include this for safety\n",
    "]\n",
    "for c in cols_to_drop:\n",
    "    if c in item_df.columns:\n",
    "        item_df = item_df.drop(c)\n",
    "\n",
    "print(\"Columns after drop:\", item_df.columns)\n",
    "\n",
    "# Define ML Stages (with handleInvalid='keep') ---\n",
    "indexer_unit = StringIndexer(\n",
    "    inputCol=\"size_unit\",\n",
    "    outputCol=\"size_unit_indexed\",\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "encoder_unit = OneHotEncoder(\n",
    "    inputCol=\"size_unit_indexed\",\n",
    "    outputCol=\"size_unit_encoded\"\n",
    ")\n",
    "\n",
    "indexer_brand = StringIndexer(\n",
    "    inputCol=\"brand\",\n",
    "    outputCol=\"brand_indexed\",\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "indexer_type = StringIndexer(\n",
    "    inputCol=\"type\",\n",
    "    outputCol=\"type_indexed\",\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"type_indexed\"],  # include it\n",
    "    outputCol=\"type_vec\"\n",
    ")\n",
    "\n",
    "\n",
    "# Execute the Pipeline ---\n",
    "pipeline = Pipeline(stages=[\n",
    "    indexer_unit,\n",
    "    encoder_unit,\n",
    "    indexer_brand,\n",
    "    indexer_type,\n",
    "    assembler\n",
    "])\n",
    "pipeline_model = pipeline.fit(item_df)\n",
    "item_df = pipeline_model.transform(item_df)\n",
    "\n",
    "# --- 6. Final DataFrame (dropping only unnecessary columns) ---\n",
    "item_df_silver = item_df.drop(\"size_unit_indexed\")\n",
    "\n",
    "#print(\"Final columns:\")\n",
    "#print(item_df_silver.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308d0ccb-7959-4bea-98be-41861545f7d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write parquet to adls"
    }
   },
   "outputs": [],
   "source": [
    "# Write to silver container\n",
    "output_path = \"/mnt/silver/items/\"\n",
    "\n",
    "item_df_silver.write.parquet(\n",
    "    output_path,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "items",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
