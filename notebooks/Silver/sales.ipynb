{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fbcb74-2471-425b-97c0-7bee2e7c81ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "load raw sales csv"
    }
   },
   "outputs": [],
   "source": [
    "ales_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"multiLine\", True)\n",
    "    .csv(\"/mnt/raw-bronze/sales.csv\")\n",
    ")\n",
    "#display(sales_df)\n",
    "sales_df.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87d621b8-c6a8-4359-9060-5c4d35d4768a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "format providence for machine learning"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col, lit\n",
    "\n",
    "# One-Hot Encode 'province' using simple conditions\n",
    "sales_df = sales_df.withColumn(\n",
    "    \"province_1\", \n",
    "    when(col(\"province\") == 1, lit(1)).otherwise(lit(0))\n",
    ")\n",
    "\n",
    "sales_df = sales_df.withColumn(\n",
    "    \"province_2\", \n",
    "    when(col(\"province\") == 2, lit(1)).otherwise(lit(0))\n",
    ")\n",
    "\n",
    "# Display the result in Databricks\n",
    "#display(sales_df.select(\"province\", \"province_1\", \"province_2\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbd24aa5-cf43-4ae2-a1d1-5ed28718dfca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "time formatting for machine learning"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sin, cos, pi, floor, lpad, concat, substring, lit, when\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "\n",
    "# (Cyclical Encoding)\n",
    "\n",
    "# Convert the integer 'time' (e.g., 1100) into 'hour' (e.g., 11)\n",
    "# Method: Convert to string, pad with zeros (e.g., 900 -> '0900'), take the first two chars (hour).\n",
    "sales_df = sales_df.withColumn(\"time_str\", lpad(col(\"time\").cast(\"string\"), 4, \"0\"))\n",
    "sales_df = sales_df.withColumn(\"hour\", col(\"time_str\").substr(1, 2).cast(IntegerType()))\n",
    "\n",
    "# 2. Calculate Cyclical Sine and Cosine features\n",
    "HOURS_IN_DAY = 24\n",
    "sales_df = sales_df.withColumn(\n",
    "    \"hour_sin\", \n",
    "    sin(2 * pi() * col(\"hour\") / lit(HOURS_IN_DAY))\n",
    ")\n",
    "sales_df = sales_df.withColumn(\n",
    "    \"hour_cos\", \n",
    "    cos(2 * pi() * col(\"hour\") / lit(HOURS_IN_DAY))\n",
    ")\n",
    "\n",
    "# Day' Column (Weekly Cycle Index and OHE) ---\n",
    "\n",
    "# Calculate the Weekly Cycle Index (1=Start of week, 7=End of week)\n",
    "# Index = ((day - 1) % 7) + 1\n",
    "sales_df = sales_df.withColumn(\n",
    "    \"day_cycle_index\", \n",
    "    (col(\"day\") - lit(1)) % lit(7) + lit(1)\n",
    ")\n",
    "\n",
    "# One-Hot Encode the Weekly Cycle Index (1 through 7)\n",
    "for i in range(1, 8):\n",
    "    sales_df = sales_df.withColumn(\n",
    "        f\"cycle_day_{i}\",\n",
    "        when(col(\"day_cycle_index\") == i, lit(1)).otherwise(lit(0))\n",
    "    )\n",
    "\n",
    "# --- Display the Transformed Features ---\n",
    "print(\"--- Sales DataFrame with Optimized Temporal Features (PySpark) ---\")\n",
    "sales_df.select(\n",
    "    \"code\", \"amount\", \"units\", \"customerId\", \"province\", \"province_1\", \"province_2\",\"supermarket\", \"basket\", \"day\", \"time\", \"hour\", \"hour_sin\", \"hour_cos\",\n",
    "    \"day_cycle_index\", \"cycle_day_1\", \"cycle_day_7\" # Showing Day 1 and Day 7 for brevity\n",
    ").limit(10).show()\n",
    "\n",
    "# NOTE: sales_df now contains the new, optimized columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308d0ccb-7959-4bea-98be-41861545f7d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write parquet to adls"
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame to the mounted 'silver' path\n",
    "output_path = \"/mnt/silver/sales/\"\n",
    "\n",
    "sales_df.write.parquet(\n",
    "    output_path,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "sales",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
