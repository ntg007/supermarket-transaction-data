{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39fbcb74-2471-425b-97c0-7bee2e7c81ff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "load raw sales csv"
    }
   },
   "outputs": [],
   "source": [
    "promotion_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"multiLine\", True)\n",
    "    .csv(\"/mnt/raw-bronze/promotion.csv\")\n",
    ")\n",
    "#display(promotion_df)\n",
    "promotion_df.createOrReplaceTempView(\"promotion\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f1bc695-b032-4b2e-9ab7-24d9de0be7c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# clean and Prepare Categorical Columns ---\n",
    "\n",
    "# Fill any null or missing string values with 'NONE' for safe indexing\n",
    "promotion_df = promotion_df.fillna(\"NONE\", subset=[\"feature\", \"display\"])\n",
    "\n",
    "# String Indexing for Categorical Features ---\n",
    "\n",
    "# Indexer for the 'feature' column (e.g., 'Interior Page Feature', 'Not on Feature')\n",
    "indexer_feature = StringIndexer(\n",
    "    inputCol=\"feature\",\n",
    "    outputCol=\"feature_indexed\",\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "# Indexer for the 'display' column (e.g., 'Mid-Aisle End Cap', 'Not on Display')\n",
    "indexer_display = StringIndexer(\n",
    "    inputCol=\"display\",\n",
    "    outputCol=\"display_indexed\",\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"display_indexed\"], \n",
    "    outputCol=\"type_vec\"\n",
    ")\n",
    "\n",
    "# Execute Pipeline\n",
    "pipeline_promo = Pipeline(stages=[indexer_feature, indexer_display, assembler])\n",
    "pipeline_model_promo = pipeline_promo.fit(promotion_df)\n",
    "promotion_df_silver = pipeline_model_promo.transform(promotion_df)\n",
    "\n",
    "promotion_df_silver.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "308d0ccb-7959-4bea-98be-41861545f7d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "write parquet to adls"
    }
   },
   "outputs": [],
   "source": [
    "# Write the DataFrame to the mounted 'silver' path\n",
    "output_path = \"/mnt/silver/promotion/\"\n",
    "\n",
    "promotion_df_silver.write.parquet(\n",
    "    output_path,\n",
    "    mode=\"overwrite\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "promotion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
