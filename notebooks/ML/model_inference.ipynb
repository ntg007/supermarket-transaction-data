{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d23f43e-4f4d-44e7-88dc-9dd1a45d2a08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Use the Databricks mount point for silver data\n",
    "base_path = \"/mnt/silver/\"\n",
    "\n",
    "# Define table configurations\n",
    "tables = {\n",
    "    \"promotion\": \"promotion/\",\n",
    "    \"item\": \"items/\",\n",
    "    \"sales\": \"sales/\",\n",
    "    \"supermarkets\": \"supermarkets/\"  \n",
    "}\n",
    "\n",
    "# Create dataframes and views in a loop\n",
    "dataframes = {}\n",
    "for table_name, folder in tables.items():\n",
    "    df = (\n",
    "        spark.read\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .option(\"multiLine\", True)\n",
    "        .parquet(f\"{base_path}{folder}\")\n",
    "    )\n",
    "    \n",
    "    # Store in dictionary for later use\n",
    "    dataframes[table_name] = df\n",
    "    \n",
    "    # Create temp view\n",
    "    df.createOrReplaceTempView(table_name)\n",
    "    \n",
    "    print(f\"Created view: {table_name}\")\n",
    "\n",
    "promotion_df = dataframes[\"promotion\"]\n",
    "item_df = dataframes[\"item\"]\n",
    "sales_df = dataframes[\"sales\"]\n",
    "supermarkets_df = dataframes[\"supermarkets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "884419a2-4b4b-4d07-bf25-57caea983434",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Join Sales and Item Data (creates 'combined_1' view) ---\n",
    "combined_df_1 = spark.sql(\"\"\"\n",
    "    SELECT s.*, i.type_indexed, i.brand_indexed, i.size_value, i.size_unit_encoded\n",
    "    FROM sales s\n",
    "    LEFT JOIN item i ON s.code = i.code\n",
    "\"\"\")\n",
    "combined_df_1.createOrReplaceTempView(\"combined_1\")\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Join Supermarket and Promotion Data (creates final featured DF) ---\n",
    "combined_featured_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c1.*, \n",
    "        sm.postal_code_indexed,\n",
    "        P.feature_indexed AS promo_feature_indexed,\n",
    "        p.display_indexed as promo_display_indexed\n",
    "    FROM combined_1 c1\n",
    "    \n",
    "    -- LEFT JOIN to Supermarkets\n",
    "    LEFT JOIN supermarkets sm \n",
    "        ON c1.supermarket = sm.supermarket\n",
    "        \n",
    "    -- LEFT JOIN to Promotion Data to get promo_feature_indexed\n",
    "    LEFT JOIN promotion P \n",
    "        ON c1.code = P.code AND\n",
    "           c1.supermarket = P.supermarkets\n",
    "          \n",
    "\"\"\")\n",
    "combined_featured_df.createOrReplaceTempView(\"combined_featured_df_cleaned\") # Renaming the view for the final step\n",
    "\n",
    "print(\"âœ… Step 2/3: Added Supermarket and Promotion features.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb0df511-8b53-46fe-985e-da0895b7921b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit, count, when, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# --- 1. INSPECT NULLS (Recommended Check) ---\n",
    "# Check for nulls in the new item features introduced by the join\n",
    "print(\"--- Null Counts Before Imputation ---\")\n",
    "combined_featured_df.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in \n",
    "     [\"type_indexed\", \"brand_indexed\", \"size_value\", \"promo_feature_indexed\", \"promo_display_indexed\"]]\n",
    ").show()\n",
    "\n",
    "\n",
    "# --- 2. NULL HANDLING (Imputation) ---\n",
    "\n",
    "# Define fill values:\n",
    "# - Indexed Features (Categorical): Fill with a unique, non-existent index (e.g., 9999.0) \n",
    "#   to represent 'UNKNOWN/MISSING' category. These must be double/float type.\n",
    "FILL_INDEXED = 9999.0 \n",
    "# - size_value (Continuous): Fill with 0.0. \n",
    "FILL_SIZE_VALUE = 0.0\n",
    "\n",
    "# Apply the null filling to the item-related features\n",
    "combined_featured_df_cleaned = combined_featured_df.fillna({\n",
    "    # Fill indexed features with the sentinel value \n",
    "    'type_indexed': FILL_INDEXED,\n",
    "    'brand_indexed': FILL_INDEXED,\n",
    "    # Fill continuous feature with 0.0\n",
    "    'size_value': FILL_SIZE_VALUE,\n",
    "    #'postal_code_indexed': FILL_INDEXED,\n",
    "    'promo_feature_indexed': FILL_INDEXED,\n",
    "    'promo_display_indexed': FILL_INDEXED\n",
    "})\n",
    "\n",
    "\n",
    "# --- 3. VERIFY AND PROCEED ---\n",
    "\n",
    "# Ensure the columns are cast back to the expected type if necessary (Spark often handles this)\n",
    "combined_featured_df_cleaned = combined_featured_df_cleaned\\\n",
    "    .withColumn(\"type_indexed\", col(\"type_indexed\").cast(DoubleType())) \\\n",
    "    .withColumn(\"brand_indexed\", col(\"brand_indexed\").cast(DoubleType()))\n",
    "\n",
    "print(\"--- Verification: Null Counts After Imputation ---\")\n",
    "combined_featured_df_cleaned.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in [\"type_indexed\", \"brand_indexed\", \"size_value\"]]\n",
    ").show()\n",
    "\n",
    "# You can now proceed with your scenario analysis using the 'combined_featured_df_cleaned' DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2b0b387-724b-44f2-b913-12a3b5e26ced",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1760645553706}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(combined_featured_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803b73bc-850d-468c-8f11-6fb7c4c85553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressionModel \n",
    "\n",
    "# Use the Databricks mount point for the model\n",
    "model_path = \"/mnt/model/sales_rf_model_v1\"\n",
    "\n",
    "try:\n",
    "    loaded_model = RandomForestRegressionModel.load(model_path)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Could not load model. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5033fb4a-06b2-4415-9207-a8e93252e050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# --- ASSUMPTIONS & PREREQUISITES ---\n",
    "# TARGET_PRODUCT_CODE: An item 'code' (e.g., a high seller).\n",
    "# -----------------------------------\n",
    "\n",
    "# 1. DEFINE YOUR FEATURE LIST (CRITICAL STEP)\n",
    "# This list MUST match the input columns used for model training EXACTLY.\n",
    "FEATURE_COLUMNS = [\n",
    "    # Sales/Temporal Features\n",
    "    \"hour_sin\", \"hour_cos\", \n",
    "    \"voucher\",\n",
    "    \"province_1\", \"province_2\", \n",
    "    \"cycle_day_1\", \"cycle_day_2\", \"cycle_day_3\", \"cycle_day_4\",\n",
    "    \"cycle_day_5\", \"cycle_day_6\", \"cycle_day_7\",\n",
    "    \n",
    "    # Item Features (Where the NaN is likely coming from due to failed extraction or join)\n",
    "    \"size_value\",\n",
    "    \"size_unit_encoded\", \n",
    "    \"brand_indexed\",     \n",
    "    \"type_indexed\",      \n",
    "    \n",
    "    # Supermarket/Promotion Features\n",
    "    \"postal_code_indexed\", \n",
    "    \"promo_feature_indexed\",\n",
    "    \"promo_display_indexed\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# --- SCENARIO SETUP ---\n",
    "\n",
    "TARGET_PRODUCT_CODE = 7546203734 # Use a product code you know is relevant\n",
    "baseline_df = combined_featured_df_cleaned.filter(col(\"code\") == TARGET_PRODUCT_CODE).limit(100)\n",
    "\n",
    "# 2. Create the BASELINE Scenario (All promotions OFF)\n",
    "baseline_scenario = baseline_df.withColumn(\"voucher\", lit(0)) \\\n",
    "                               .withColumn(\"promo_feature_indexed\", lit(0)) \\\n",
    "                               .withColumn(\"Scenario\", lit(\"Baseline\"))\n",
    "\n",
    "# 3. Create the VOUCHER Scenario (Voucher ON)\n",
    "voucher_scenario = baseline_df.withColumn(\"voucher\", lit(1)) \\\n",
    "                              .withColumn(\"promo_feature_indexed\", lit(0)) \\\n",
    "                              .withColumn(\"Scenario\", lit(\"Voucher\"))\n",
    "\n",
    "# 4. Combine Scenarios\n",
    "combined_scenarios = baseline_scenario.unionAll(voucher_scenario)\n",
    "\n",
    "\n",
    "# --- FEATURE ASSEMBLING & PREDICTION ---\n",
    "\n",
    "# 5. Assemble Features: This creates the single 'features' vector column required by the model.\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURE_COLUMNS,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "combined_scenarios_vect = assembler.transform(combined_scenarios)\n",
    "\n",
    "# 6. Run Prediction using the loaded model\n",
    "promotion_analysis_df = loaded_model.transform(combined_scenarios_vect)\n",
    "\n",
    "# 7. Aggregate Results\n",
    "promotion_results = promotion_analysis_df.groupBy(\"Scenario\") \\\n",
    "    .agg({\"prediction\": \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(prediction)\", \"Average_Predicted_Sales\") \\\n",
    "    .orderBy(\"Average_Predicted_Sales\", ascending=False)\n",
    "\n",
    "print(\"\\n--- Promotion Effectiveness 'What-If' Analysis Results ---\")\n",
    "promotion_results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ecd570f-6960-48a3-9c78-0f0cafe7d28d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# --- ASSUMPTIONS & PREREQUISITES ---\n",
    "# Assuming 'loaded_model' (your best model) and 'combined_featured_df_cleaned' \n",
    "# (your final feature DataFrame) are available.\n",
    "# -----------------------------------\n",
    "\n",
    "# 1. Reuse the base filtered data for a specific product\n",
    "TARGET_PRODUCT_CODE = 7546203734\n",
    "# We use a base DataFrame to ensure all non-promotion features (time, location, size) are realistic\n",
    "base_df = combined_featured_df_cleaned.filter(col(\"code\") == TARGET_PRODUCT_CODE).limit(100)\n",
    "\n",
    "# 2. Define All Scenarios (assuming 0.0 is 'Not on Promo' and 1.0 is a typical promotion index)\n",
    "\n",
    "# A. 01. Baseline Scenario (All promotions OFF)\n",
    "baseline_scenario = base_df.withColumn(\"voucher\", lit(0)) \\\n",
    "                          .withColumn(\"promo_feature_indexed\", lit(0.0)) \\\n",
    "                          .withColumn(\"promo_display_indexed\", lit(0.0)) \\\n",
    "                          .withColumn(\"Scenario\", lit(\"01_Baseline_OFF\"))\n",
    "\n",
    "# B. 02. Voucher Scenario (Voucher ON, others OFF)\n",
    "voucher_scenario = base_df.withColumn(\"voucher\", lit(1)) \\\n",
    "                         .withColumn(\"promo_feature_indexed\", lit(0.0)) \\\n",
    "                         .withColumn(\"promo_display_indexed\", lit(0.0)) \\\n",
    "                         .withColumn(\"Scenario\", lit(\"02_Voucher_Only\"))\n",
    "\n",
    "# C. 03. Feature Promo Scenario (e.g., Item featured in a flyer, others OFF)\n",
    "feature_scenario = base_df.withColumn(\"voucher\", lit(0)) \\\n",
    "                         .withColumn(\"promo_feature_indexed\", lit(1.0)) \\\n",
    "                         .withColumn(\"promo_display_indexed\", lit(0.0)) \\\n",
    "                         .withColumn(\"Scenario\", lit(\"03_Feature_Only\"))\n",
    "\n",
    "# D. 04. Display Promo Scenario (e.g., End Cap Display ON, others OFF)\n",
    "display_scenario = base_df.withColumn(\"voucher\", lit(0)) \\\n",
    "                         .withColumn(\"promo_feature_indexed\", lit(0.0)) \\\n",
    "                         .withColumn(\"promo_display_indexed\", lit(1.0)) \\\n",
    "                         .withColumn(\"Scenario\", lit(\"04_Display_Only\"))\n",
    "\n",
    "# E. 05. Maximum Scenario (All promotions ON)\n",
    "max_scenario = base_df.withColumn(\"voucher\", lit(1)) \\\n",
    "                      .withColumn(\"promo_feature_indexed\", lit(1.0)) \\\n",
    "                      .withColumn(\"promo_display_indexed\", lit(1.0)) \\\n",
    "                      .withColumn(\"Scenario\", lit(\"05_Max_Combined\"))\n",
    "\n",
    "# 3. Combine All Scenarios\n",
    "combined_scenarios = baseline_scenario.unionAll(voucher_scenario) \\\n",
    "                                      .unionAll(feature_scenario) \\\n",
    "                                      .unionAll(display_scenario) \\\n",
    "                                      .unionAll(max_scenario)\n",
    "\n",
    "# 4. Feature Assembler (MUST match original training features)\n",
    "FEATURE_COLUMNS = [\n",
    "    \"hour_sin\", \"hour_cos\", \"voucher\", \"province_1\", \"province_2\", \n",
    "    \"cycle_day_1\", \"cycle_day_2\", \"cycle_day_3\", \"cycle_day_4\",\n",
    "    \"cycle_day_5\", \"cycle_day_6\", \"cycle_day_7\",\n",
    "    \"size_value\", \"size_unit_encoded\", \"brand_indexed\", \"type_indexed\",      \n",
    "    \"postal_code_indexed\", \"promo_feature_indexed\", \"promo_display_indexed\"\n",
    "]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=FEATURE_COLUMNS,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "combined_scenarios_vect = assembler.transform(combined_scenarios)\n",
    "\n",
    "# 5. Run Prediction using the loaded model (assuming it's loaded as loaded_model)\n",
    "promotion_analysis_df = loaded_model.transform(combined_scenarios_vect)\n",
    "\n",
    "# 6. Aggregate Results\n",
    "promotion_results = promotion_analysis_df.groupBy(\"Scenario\") \\\n",
    "    .agg({\"prediction\": \"avg\"}) \\\n",
    "    .withColumnRenamed(\"avg(prediction)\", \"Average_Predicted_Sales\") \\\n",
    "    .orderBy(\"Scenario\", ascending=True) # Order by Scenario number for clean viewing\n",
    "\n",
    "print(\"\\n--- Expanded Promotion Effectiveness Analysis Results ---\")\n",
    "promotion_results.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6348956669364763,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "model_inference",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
